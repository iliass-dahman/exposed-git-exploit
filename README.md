# exposed-git-exploit
## exploit
## Scraping using selenium
There are many libraries for scraping, we used Selenium because it is easy to use and can be used in various environments, it is mostlly used for browser automation, and it provides a variety of ways to interact with the browser such as clicking, scrolling, executing custom Js code, etc. Selenium requires a Webdriver to interact with the browser, for this project we used ChromeDriver.

Now with the interesting part, we will use Selenium to scrape the exposed git repositories, for that we should introduce google dorks, google dorks are use for advanced google search, it can be used to search for specific files, specific file types, specific file names, etc. One of the dorks we used is `index of ".git"`, this dork will search for all websites that have a .git directory, so we will use this dork to search for exposed git folders, and then we will scrape the exposed git folder using Selenium, we do the same for svn, bazaar etc. For each page we store the websites ended with `.git`, `.svn`, `.bzr` etc. in a list and we navigate to the next page and so on, until we reach the last page, then we save the list in a file.

## bypassing-captcha
### using google's AI for speech to text conversion
As we are scraping google, we should expect to face captcha, and try to bypass it. In our case, we thought of two ways to bypass it, at first we tried to solve it by downloading the audio file and then we used the google speech recognition library to convert the audio to text, and then submit it, that works perfectly at first, but then we faced a problem, google will block us from getting the audio file after we solve the captcha for many times, so we had to find another way, the second one, we simply switched to another proxy if we didn't get the audio file, and we will not face captcha for that request. In this method we used `scraperapi` to switch between proxies.

## use threads
Now everything is working fine, but it is very slow, to go through all the pages and scrape all the exposed git folders, so we used threads to speed up the process. Basically a thread is a process that runs in parallel to the main process, and it can be used to do multiple tasks at the same time. We used `concurrent.futures` to create threads, and we used `ThreadPoolExecutor` to create a pool of threads, and we used `submit` to submit a task to the pool.

To make things more interesting, we come up with an idea to allow user specify a number of search queries, and a number of threads that will handel each query, for example he can specify 10 queries, and 10 threads for them or 5 threads each one will be responsible for 2 queries, and so on. He then should specify how many threads will scrape each query. At the end we will have n threads each one handels the work for m other threads, each one of them responsible of scraping a slice of the search results.