import csv
import os
from time import sleep
from urllib.parse import urlencode

from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

import process_captcha

# load api key
# needs an .env file in /gdorks/
load_dotenv()
# Global variables
options = Options()
websites = []


def get_scraperapi_url(url):
    payload = {'api_key': os.getenv("SCRAPERAPI_KEY"), 'url': url}
    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)
    return proxy_url


# global start url
url = get_scraperapi_url("https://www.google.com/search?q=%22.git%22+index+of")


# Prepare the driver
def prepare_driver():
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("--window-size=1920,1200")
    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
    return driver


# Flip the page
def flip_page(driver):
    try:
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
        sleep(2)
        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID, 'pnnext')))
        next_page = driver.find_element(By.ID, "pnnext")
        link = next_page.get_attribute("href")
        link = link.replace("http://api.scraperapi.com", "https://www.google.com")
        driver.get(get_scraperapi_url(link))
        process_captcha.bypass_captcha(driver)

        return True
    except Exception as e:
        print("No more pages", e)
        # remove next line in prod
        driver.get(url)
        return False


# Scan the search results
def scan(url):
    driver = prepare_driver()
    driver.get(url)
    print("Scanning", url)
    while True:
        containers = driver.find_elements(By.CLASS_NAME, "yuRUbf")
        for container in containers:
            full_link = container.find_element(By.TAG_NAME, "a").get_attribute("href")
            if "/.git/" in full_link:
                short_link = full_link.split("/.git/")[0]
                websites.append([short_link, full_link])
        print("Now", len(websites), " websites")
        if not flip_page(driver):
            break
    driver.close()


# Show the results
def show(websites):
    print("Found", len(websites), "websites")
    for website in websites:
        print(website)


# Storing the results in a file
def store(websites):
    fields = ['website', '.git link']
    os.makedirs(os.path.dirname('./scraping/results.csv'), exist_ok=True)
    with open('./scraping/results.csv', 'w+') as f:
        write = csv.writer(f)
        write.writerow(fields)
        write.writerows(websites)


def main():
    scan(url)
    store(websites)


main()
