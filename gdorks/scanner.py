import csv
import os
from time import sleep
from urllib.parse import urlencode
from config import searchList

from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

import process_captcha

# load api key
# needs an .env file in /gdorks/
load_dotenv()
# Global variables
options = Options()

## variable to tell if the bot is already detected
## aka there was a time in the past when no captcha was given by google
## and we were forbidden to continue the scrapping
detected: bool = False


def update_detected(new_value: bool):
    global detected
    detected = new_value


def get_scraperapi_url(url):
    if not detected:
        return url
    payload = {'api_key': os.getenv("SCRAPERAPI_KEY"), 'url': url}
    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)
    return proxy_url


def convert_to_scraperapi_url(old_url: str) -> str:
    if not detected:
        return old_url
    return old_url.replace("http://api.scraperapi.com", "https://www.google.com")


# global start url
url = get_scraperapi_url("https://www.google.com/search?q=%22.git%22+index+of")


# Prepare the driver
def prepare_driver():
    options = Options()
    #options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("--window-size=1920,1200")
    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
    return driver


# Flip the page
def flip_page(driver):
    try:
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
        sleep(2)
        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID, 'pnnext')))
        next_page = driver.find_element(By.ID, "pnnext")
        link = next_page.get_attribute("href")
        ## gets same link if not yet detected otherwise the proxy embedded link
        link = convert_to_scraperapi_url(link)
        driver.get(get_scraperapi_url(link))
        found = process_captcha.bypass_captcha(driver)
        if not found:
            update_detected(False)

        return True
    except Exception as e:
        print("No more pages or")
        print(e)
        # remove next line in prod
        #driver.get(url)
        return False


# Scan the search results
def scan(url,websites):
    driver = prepare_driver()
    driver.get(url)
    print("Scanning", url)
    while True:
        containers = driver.find_elements(By.CLASS_NAME, "yuRUbf")
        for container in containers:
            full_link = container.find_element(By.TAG_NAME, "a").get_attribute("href")
            if "/.git/" in full_link:
                short_link = full_link.split("/.git/")[0]
                websites.append([short_link, full_link])
        print("Now", len(websites), " websites")
        if not flip_page(driver):
            break
    driver.close()

# Prepare the search query
def prepare_search_query(query,current_page):
    return "https://www.google.com/search?q=" + query + "&start=" + str(current_page*10)

# Filter the results
def filter_link(link,websites):
    for item in searchList:
        if item['indicator'] in link:
            websites.append([
                item['type'],
                link
                ])
            break
# Check if page not found
def page_not_found(driver):
    try:
        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.CLASS_NAME, 'jHxzxc')))
        print("Page not found")
        return True
    except Exception as e:
        return False

# Advanced scanner with multithreading
def scan_by_slice(query,start,end,websites):
    currPage = start
    currLink = prepare_search_query(query,currPage)
    driver = prepare_driver()
    driver.get(currLink)
    print("Scanning", currLink)
    if process_captcha.check_captcha(driver):
        bypassed=process_captcha.bypass_captcha(driver)
        update_detected(not bypassed)
    while True:
        if page_not_found(driver):
            break
        containers = driver.find_elements(By.CLASS_NAME, "yuRUbf")
        for container in containers:
            full_link = container.find_element(By.TAG_NAME, "a").get_attribute("href")
            filter_link(full_link)
        print("Now", len(websites), " websites at page", currPage, "with query", query)
        if not flip_page(driver) or currPage == end:
            break
        currPage += 1
    print("Finished scanning")
    try:
        driver.close()
    except Exception as e:
        print(e)
    return websites

# Show the results
def show(websites):
    print("Found", len(websites), "websites")
    for website in websites:
        print(website)


# Storing the results in a file
def store(websites):
    print("Storing the results")
    fields = ['type', 'link']
    os.makedirs(os.path.dirname('./scraping/results.csv'), exist_ok=True)
    with open('./scraping/results.csv', 'w+') as f:
        write = csv.writer(f)
        write.writerow(fields)
        write.writerows(websites)


def main():
    scan(url)
    #store(websites)


#main()
