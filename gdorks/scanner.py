import csv
import os
from time import sleep
from attr import fields
from numpy import short
from selenium import webdriver
from sympy import true
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

# Global variables
options= Options()
url= "https://www.google.com/search?q=%22.git%22+index+of"
websites= []

# Prepare the driver
def prepare_driver():
    options = Options()
    # options.add_argument('--headless')
    # options.add_argument('--no-sandbox')
    # options.add_argument('--disable-dev-shm-usage')
    options.add_argument("--window-size=1920,1200")
    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
    return driver

#check for captcha
def check_captcha(driver):
    try:
        driver.find_element(By.ID, "infoDiv")
        print("Captcha detected")
        return True
    except:
        print("No captcha")
        return False

#Bypassing the captcha
def bypass_captcha(driver):
    while check_captcha(driver):
        sleep(5)
        print("Captcha detected")
        driver.refresh()
        sleep(5)

# Flip the page
def flip_page(driver):
    try:
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
        sleep(5)
        next_page= driver.find_element(By.ID, "pnnext")
        driver.get(next_page.get_attribute("href"))
        bypass_captcha(driver)
        return True
    except:
        print("No more pages")
        return False

# Scan the search results
def scan(url):
    driver = prepare_driver()
    driver.get(url)
    print("Scanning", url)
    sleep(5)
    while True:
        containers= driver.find_elements(By.CLASS_NAME, "yuRUbf")
        for container in containers:
            full_link = container.find_element(By.TAG_NAME, "a").get_attribute("href")
            if "/.git/" in full_link:
                short_link = full_link.split("/.git/")[0]
                websites.append([short_link, full_link])
        sleep(1)
        print("Now", len(websites))
        if not flip_page(driver):
            break
    sleep(1)
    driver.close()

# Show the results
def show(websites):
    print("Found", len(websites), "websites")
    for website in websites:
        print(website)

#Storing the results in a file
def store(websites):
    fields = ['website', '.git link']
    os.makedirs(os.path.dirname('./scraping/results.csv'), exist_ok=True)
    with open('./scraping/results.csv', 'w+') as f:
        write= csv.writer(f)
        write.writerow(fields)
        write.writerows(websites)

def main():
    scan(url)
    store(websites)
main()
