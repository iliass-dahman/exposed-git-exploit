import concurrent.futures
import csv
import os
from threading import current_thread
from time import sleep
from config import *
from scanner import scan_by_slice

webSites = []

def google_search(query):
    with concurrent.futures.ThreadPoolExecutor(max_workers=searchWorkers) as executor:
        results = executor.map(scan, [i for i in range(0,int(maxPages/slices))],[query['query']]*slices)

def scan(index,query):
    start = index * slices
    end = start + slices - 1
    print("##searching for",query," starting from: ",start," to: ",end)
    res=scan_by_slice(query,start,end,[])
    webSites.extend(res)


# Storing the results in a file
def store(websites):
    print("Storing the results")
    fields = ['type', 'link']
    os.makedirs(os.path.dirname('./scraping/results.csv'), exist_ok=True)
    with open('./scraping/results.csv', 'w+') as f:
        write = csv.writer(f)
        write.writerow(fields)
        write.writerows(websites)
    

with concurrent.futures.ThreadPoolExecutor(max_workers=masterWorkers) as executor:
    results = executor.map(google_search, searchList)
    for result in results:
        print("##")
    print("Finished scanning")
    store(webSites)


