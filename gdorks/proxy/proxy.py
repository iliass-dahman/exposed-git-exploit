################################################################
## JUNK CODE
## USED ONLY FOR TESTING PURPOSES
################################################################
from urllib.parse import urlencode

from fp.fp import FreeProxy
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
# from seleniumwire import webdriver
from webdriver_manager.chrome import ChromeDriverManager

freeProxy = FreeProxy(google=True).get_proxy_list()
print(freeProxy)
proxy = ""
freeProxy[0] = "11.11.11.111:8080"
counter = 0


def get_proxy():
    global counter, freeProxy
    if counter >= len(freeProxy):
        # check if the lists are the same or something has changed
        freeProxy = list(set(FreeProxy(google=True, https=True).get_proxy_list()) - set(freeProxy))
        return get_proxy()
    counter += 1
    return freeProxy[counter - 1]


def prepare_driver():
    proxy_options = {
        'proxy': {
            'http': f'http://scraperapi:API_KEY@proxy-server.scraperapi.com:8001',
            'no_proxy': 'localhost,127.0.0.1'
        }
    }
    options = Options()
    # options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("--window-size=1920,1200")
    # proxy = freeProxy.get()
    # global proxy
    # proxy = "103.23.236.206:8080"#get_proxy()
    # options.add_argument('--proxy-server=%s' % proxy)
    # print("trying proxy : ", proxy)
    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
    return driver


def get_scraperapi_url(url):
    """
        Converts url into API request for ScraperAPI.
    """
    payload = {'api_key': "API_KEY", 'url': url}
    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)
    return proxy_url


# driver
def main():
    connected = False
    driver = prepare_driver()
    while (not connected):
        try:
            driver.get(
                get_scraperapi_url("http://httpbin.org/ip"))
            # "https://www.google.com/search?q=fdsq&sxsrf=ALiCzsb89kOxczLf46K_be--0NuyOaKo2w%3A1666188258508&source=hp&ei=4gNQY9SIG--Jur4PlZKQ2Ak&iflsig=AJiK0e8AAAAAY1AR8voMZpiYTvVGcNofFB6BQSLs39LG&ved=0ahUKEwjUgZj6uuz6AhXvhM4BHRUJBJsQ4dUDCAg&uact=5&oq=fdsq&gs_lp=Egdnd3Mtd2l6uAED-AEBMgQQIxgnMgQQABhDMgoQLhjHARjRAxhDMgQQABhDMgQQABhDMgQQABhDMgQQABhDMgsQLhiABBjHARjRAzIFEAAYgAQyBRAAGIAEwgIFEAAYkQJIgQJQAFiaAXAAeADIAQCQAQCYAZsBoAH_AaoBAzAuMg&sclient=gws-wiz")
            # print(driver.find_elements(By.CLASS_NAME, "error-code"))
            # if driver.find_element(By.ID, "search"):
            #     connected = True
            #     print(driver.page_source)
        except:
            print()
            # driver.close()
            # driver = prepare_driver()

    print(proxy)


if __name__ == '__main__':
    main()

# proxies = {
#     "http": "http://scraperapi:API_KEY@proxy-server.scraperapi.com:8001"
# }
# print(proxies)
